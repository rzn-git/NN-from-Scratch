{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
      "Iteration 0, Cost: 2.3016256293850863\n",
      "Iteration 100, Cost: 2.2823703153740986\n",
      "Iteration 200, Cost: 2.133786625070115\n",
      "Iteration 300, Cost: 1.6736854297719395\n",
      "Iteration 400, Cost: 1.246369003642687\n",
      "Iteration 500, Cost: 0.9788800608567569\n",
      "Iteration 600, Cost: 0.8093288236790459\n",
      "Iteration 700, Cost: 0.6973533066655225\n",
      "Iteration 800, Cost: 0.6202771813094639\n",
      "Iteration 900, Cost: 0.5643193495129554\n",
      "Test Accuracy: 87.28%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "\n",
    "# Flatten the 28x28 images into vectors of size 784\n",
    "train_X = train_X.reshape(train_X.shape[0], -1) / 255.0  # Normalize input data to [0, 1]\n",
    "test_X = test_X.reshape(test_X.shape[0], -1) / 255.0\n",
    "\n",
    "# One-hot encode the labels (for 10 classes)\n",
    "def one_hot_encode(y, num_classes):\n",
    "    one_hot = np.zeros((y.size, num_classes))\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "train_y = one_hot_encode(train_y, 10)\n",
    "test_y = one_hot_encode(test_y, 10)\n",
    "\n",
    "# Activation functions and their derivatives\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))  # For numerical stability\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "# Initialize weights and biases\n",
    "input_size = 784      # Number of input neurons (28x28 pixels)\n",
    "hidden_size = 64      # Number of neurons in the hidden layer\n",
    "output_size = 10      # Number of output neurons (10 classes for digits 0-9)\n",
    "\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Weights and biases initialization\n",
    "W1 = np.random.randn(input_size, hidden_size) * 0.01  # Weights from input to hidden layer\n",
    "b1 = np.zeros((1, hidden_size))                        # Biases for hidden layer\n",
    "W2 = np.random.randn(hidden_size, output_size) * 0.01  # Weights from hidden to output layer\n",
    "b2 = np.zeros((1, output_size))                        # Biases for output layer\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(X):\n",
    "    # Hidden layer\n",
    "    Z1 = np.dot(X, W1) + b1  # Linear transformation for hidden layer\n",
    "    A1 = sigmoid(Z1)         # Activation using sigmoid function\n",
    "    \n",
    "    # Output layer\n",
    "    Z2 = np.dot(A1, W2) + b2  # Linear transformation for output layer\n",
    "    A2 = softmax(Z2)          # Activation using softmax function for output probabilities\n",
    "    \n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "# Backward propagation\n",
    "def backward_propagation(X, y, Z1, A1, Z2, A2):\n",
    "    # Number of samples\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Output layer error\n",
    "    dZ2 = A2 - y  # Error at output layer (softmax - true labels)\n",
    "    dW2 = np.dot(A1.T, dZ2) / m  # Gradient of W2\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m  # Gradient of b2\n",
    "    \n",
    "    # Hidden layer error\n",
    "    dA1 = np.dot(dZ2, W2.T)  # Backpropagated error to hidden layer\n",
    "    dZ1 = dA1 * sigmoid_derivative(A1)  # Gradient of sigmoid function\n",
    "    dW1 = np.dot(X.T, dZ1) / m  # Gradient of W1\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m  # Gradient of b1\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "# Training the network\n",
    "def train(X, y, iterations, learning_rate):\n",
    "    global W1, b1, W2, b2\n",
    "    for i in range(iterations):\n",
    "        # Forward propagation\n",
    "        Z1, A1, Z2, A2 = forward_propagation(X)\n",
    "        \n",
    "        # Backward propagation\n",
    "        dW1, db1, dW2, db2 = backward_propagation(X, y, Z1, A1, Z2, A2)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        W1 -= learning_rate * dW1\n",
    "        b1 -= learning_rate * db1\n",
    "        W2 -= learning_rate * dW2\n",
    "        b2 -= learning_rate * db2\n",
    "        \n",
    "        # Every 100 iterations, print the cost (cross-entropy loss)\n",
    "        if i % 100 == 0:\n",
    "            cost = -np.mean(np.sum(y * np.log(A2 + 1e-8), axis=1))  # Cross-entropy loss\n",
    "            print(f\"Iteration {i}, Cost: {cost}\")\n",
    "\n",
    "# Predictions\n",
    "def predict(X):\n",
    "    _, _, _, A2 = forward_propagation(X)\n",
    "    return np.argmax(A2, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "def accuracy(predictions, labels):\n",
    "    return np.mean(predictions == np.argmax(labels, axis=1))\n",
    "\n",
    "# Training the network on the MNIST dataset\n",
    "train(X=train_X, y=train_y, iterations=1000, learning_rate=0.1)\n",
    "\n",
    "# Testing accuracy on the test set\n",
    "test_predictions = predict(test_X)\n",
    "print(f\"Test Accuracy: {accuracy(test_predictions, test_y) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the class of a single test sample\n",
    "def predict_single(sample):\n",
    "    # Reshape and normalize the input sample\n",
    "    sample = sample.reshape(1, -1) / 255.0  # Reshape to (1, 784) and normalize\n",
    "    _, _, _, A2 = forward_propagation(sample)  # Forward propagation\n",
    "    predicted_class = np.argmax(A2, axis=1)  # Get the predicted class\n",
    "    return predicted_class[0]  # Return the predicted class as a single integer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class for sample 0: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the predicted class for a single test sample\n",
    "single_test_sample = test_X[200]  # Change the index to test other samples\n",
    "predicted_class = predict_single(single_test_sample)\n",
    "print(f\"Predicted Class for sample 0: {predicted_class}\")\n",
    "\n",
    "# If you want to visualize the sample, you can use matplotlib (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display the test sample\n",
    "plt.imshow(single_test_sample.reshape(28, 28), cmap='gray')\n",
    "plt.title(f\"Predicted Class: {predicted_class}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
